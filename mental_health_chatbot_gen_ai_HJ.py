# -*- coding: utf-8 -*-
"""Mental_Health_Chatbot_GEN_AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U_t3iMLt-NXEp9k7tmD4MbDFBGng323c
"""

!pip install langchain_groq langchain_core langchain_community

from langchain_groq import ChatGroq
llm = ChatGroq(
    temperature = 0,
    groq_api_key = "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",
    model_name = "llama-3.3-70b-versatile"
)
result = llm.invoke("Who is lord Ram?")
print(result.content)

!pip install pypdf

!pip install chromadb

!pip install sentence_transformers

!pip uninstall my_module
!pip install my_module

!pip install -U langchain-groq

!pip install langchain_groq langchain_core langchain_community

from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_groq import ChatGroq


import os
def initialize_llm():
  llm = ChatGroq(
    temperature = 0,
    groq_api_key = "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",
    model_name = "llama-3.3-70b-versatile"
)
  return llm

def create_vector_db():
  loader = DirectoryLoader("/content/data/", glob = '*.pdf', loader_cls = PyPDFLoader)
  documents = loader.load()
  text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 50)
  texts = text_splitter.split_documents(documents)
  embeddings = HuggingFaceBgeEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')
  vector_db = Chroma.from_documents(texts, embeddings, persist_directory = './chroma_db')
  vector_db.persist()

  print("ChromaDB created and data saved")

  return vector_db

def setup_qa_chain(vector_db, llm):
  retriever = vector_db.as_retriever()
  prompt_templates = """ You are a compassionate mental health chatbot. Respond thoughtfully to the following question:
    {context}
    User: {question}
    Chatbot: """
  PROMPT = PromptTemplate(template = prompt_templates, input_variables = ['context', 'question'])

  qa_chain = RetrievalQA.from_chain_type(
      llm = llm,
      chain_type = "stuff",
      retriever = retriever,
      chain_type_kwargs = {"prompt": PROMPT}
  )
  return qa_chain


def main():
  print("Intializing Chatbot.........")
  llm = initialize_llm()

  db_path = "/content/chroma_db"

  if not os.path.exists(db_path):
    vector_db  = create_vector_db()
  else:
    embeddings = HuggingFaceBgeEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')
    vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)
  qa_chain = setup_qa_chain(vector_db, llm)

  while True:
    query = input("\nHuman: ")
    if query.lower()  == "exit":
      print("Chatbot: Take Care of yourself, Goodbye!")
      break
    response = qa_chain.run(query)
    print(f"Chatbot: {response}")

if __name__ == "__main__":
  main()

create_vector_db()

!pip install gradio

!pip install langchain-groq

!pip install langchain-groq gradio chromadb sentence-transformers

from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_groq import ChatGroq
import os
import gradio as gr

# Initialize LLM with Groq
def initialize_llm():
    llm = ChatGroq(
        temperature=0,
        groq_api_key="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",  # Ïã§Ï†ú API ÌÇ§Î•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî
        model_name="llama2-70b-4096"  # GroqÏóêÏÑú ÏßÄÏõêÌïòÎäî Î™®Îç∏Î™ÖÏúºÎ°ú Î≥ÄÍ≤Ω
    )
    return llm

# Vector Database Creation
def create_vector_db():
    loader = DirectoryLoader("/content/data/", glob='*.pdf', loader_cls=PyPDFLoader)
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = text_splitter.split_documents(documents)

    embeddings = HuggingFaceBgeEmbeddings(
        model_name='sentence-transformers/all-MiniLM-L6-v2'
    )

    vector_db = Chroma.from_documents(
        texts,
        embeddings,
        persist_directory='./chroma_db'
    )
    vector_db.persist()

    print("ChromaDB created and data saved")
    return vector_db

# Setup QA Chain
def setup_qa_chain(vector_db, llm):
    retriever = vector_db.as_retriever()
    prompt_templates = """You are a compassionate mental health chatbot. Respond thoughtfully to the following question:
    {context}
    User: {question}
    Chatbot: """

    PROMPT = PromptTemplate(
        template=prompt_templates,
        input_variables=['context', 'question']
    )

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": PROMPT}
    )
    return qa_chain

# Chatbot Response Function
def chatbot_response(user_input, history=[]):
    if not user_input.strip():
        return "Please provide a valid input", history
    try:
        response = qa_chain.run(user_input)
        history.append((user_input, response))
        return "", history
    except Exception as e:
        return f"Error: {str(e)}", history

# Main Execution
print("Initializing Chatbot.........")
try:
    llm = initialize_llm()

    db_path = "/content/chroma_db"

    if not os.path.exists(db_path):
        vector_db = create_vector_db()
    else:
        embeddings = HuggingFaceBgeEmbeddings(
            model_name='sentence-transformers/all-MiniLM-L6-v2'
        )
        vector_db = Chroma(
            persist_directory=db_path,
            embedding_function=embeddings
        )

    qa_chain = setup_qa_chain(vector_db, llm)

    # Gradio Interface
    with gr.Blocks(theme='Respair/Shiki@1.2.1') as app:
        gr.Markdown("# üß† Mental Health Chatbot ü§ñ")
        gr.Markdown("A compassionate chatbot designed to assist with mental well-being. Please note: For serious concerns, contact a professional.")

        chatbot = gr.ChatInterface(
            fn=chatbot_response,
            title="Mental Health Chatbot"
        )

        gr.Markdown("This chatbot provides general support. For urgent issues, seek help from licensed professionals.")

    app.launch()

except Exception as e:
    print(f"Error during initialization: {str(e)}")

from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_groq import ChatGroq
import os
import gradio as gr
import logging

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize LLM with Groq
def initialize_llm():
    try:
        llm = ChatGroq(
            temperature=0.7,  # ÏùëÎãµÏùò Ï∞ΩÏùòÏÑ±ÏùÑ ÎÜíÏù¥Í∏∞ ÏúÑÌï¥ temperature Ï°∞Ï†ï
            groq_api_key="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",
            model_name="mixtral-8x7b-32768"  # Î™®Îç∏ Î≥ÄÍ≤Ω
        )
        logger.info("LLM initialized successfully")
        return llm
    except Exception as e:
        logger.error(f"Error initializing LLM: {str(e)}")
        raise

# Vector Database Creation
def create_vector_db():
    try:
        if not os.path.exists("/content/data"):
            os.makedirs("/content/data")
            logger.warning("Created /content/data directory as it didn't exist")

        loader = DirectoryLoader("/content/data/", glob='*.pdf', loader_cls=PyPDFLoader)
        documents = loader.load()

        if not documents:
            logger.warning("No documents found in the specified directory")
            return None

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        texts = text_splitter.split_documents(documents)

        embeddings = HuggingFaceBgeEmbeddings(
            model_name='sentence-transformers/all-MiniLM-L6-v2'
        )

        vector_db = Chroma.from_documents(
            texts,
            embeddings,
            persist_directory='./chroma_db'
        )
        vector_db.persist()

        logger.info("ChromaDB created and data saved successfully")
        return vector_db
    except Exception as e:
        logger.error(f"Error creating vector database: {str(e)}")
        raise

# Setup QA Chain
def setup_qa_chain(vector_db, llm):
    try:
        retriever = vector_db.as_retriever(search_kwargs={"k": 3})  # ÏÉÅÏúÑ 3Í∞ú Í≤∞Í≥º Í≤ÄÏÉâ
        prompt_templates = """You are a compassionate mental health chatbot. Based on the following context, provide a helpful and empathetic response:

        Context: {context}

        Question: {question}

        Please provide a detailed and supportive answer:"""

        PROMPT = PromptTemplate(
            template=prompt_templates,
            input_variables=['context', 'question']
        )

        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={"prompt": PROMPT}
        )
        logger.info("QA Chain setup completed")
        return qa_chain
    except Exception as e:
        logger.error(f"Error setting up QA chain: {str(e)}")
        raise

# Chatbot Response Function
def chatbot_response(user_input, history=[]):
    if not user_input.strip():
        return "Please provide a valid input", history

    try:
        logger.info(f"Processing user input: {user_input}")
        response = qa_chain.run(user_input)

        if not response:
            response = "I apologize, but I couldn't generate a proper response. Could you please rephrase your question?"

        history.append((user_input, response))
        return "", history
    except Exception as e:
        logger.error(f"Error in chatbot response: {str(e)}")
        return f"I apologize, but an error occurred: {str(e)}", history

# Main Execution
def main():
    print("Initializing Chatbot.........")
    try:
        global qa_chain  # Make qa_chain global so it can be accessed by chatbot_response
        llm = initialize_llm()

        db_path = "/content/chroma_db"

        if not os.path.exists(db_path):
            vector_db = create_vector_db()
            if vector_db is None:
                raise Exception("Failed to create vector database")
        else:
            embeddings = HuggingFaceBgeEmbeddings(
                model_name='sentence-transformers/all-MiniLM-L6-v2'
            )
            vector_db = Chroma(
                persist_directory=db_path,
                embedding_function=embeddings
            )

        qa_chain = setup_qa_chain(vector_db, llm)

        # Gradio Interface
        with gr.Blocks(theme='Respair/Shiki@1.2.1') as app:
            gr.Markdown("# üß† Mental Health Chatbot ü§ñ")
            gr.Markdown("A compassionate chatbot designed to assist with mental well-being. Please note: For serious concerns, contact a professional.")

            chatbot = gr.ChatInterface(
                fn=chatbot_response,
                title="Mental Health Chatbot",
                examples=["How can I manage stress?", "What are some relaxation techniques?"],
                retry_btn="Retry",
                undo_btn="Undo"
            )

            gr.Markdown("This chatbot provides general support. For urgent issues, seek help from licensed professionals.")

        app.launch(share=True, debug=True)

    except Exception as e:
        logger.error(f"Error during initialization: {str(e)}")
        raise

if __name__ == "__main__":
    main()

# Gradio Interface Î∂ÄÎ∂Ñ ÏàòÏ†ï
def main():
    print("Initializing Chatbot.........")
    try:
        global qa_chain
        llm = initialize_llm()

        db_path = "/content/chroma_db"

        if not os.path.exists(db_path):
            vector_db = create_vector_db()
            if vector_db is None:
                raise Exception("Failed to create vector database")
        else:
            embeddings = HuggingFaceBgeEmbeddings(
                model_name='sentence-transformers/all-MiniLM-L6-v2'
            )
            vector_db = Chroma(
                persist_directory=db_path,
                embedding_function=embeddings
            )

        qa_chain = setup_qa_chain(vector_db, llm)

        # Gradio Interface - ÏàòÏ†ïÎêú Î∂ÄÎ∂Ñ
        with gr.Blocks(theme='Respair/Shiki@1.2.1') as app:
            gr.Markdown("# üß† Mental Health Chatbot ü§ñ")
            gr.Markdown("A compassionate chatbot designed to assist with mental well-being. Please note: For serious concerns, contact a professional.")

            chatbot = gr.ChatInterface(
                fn=chatbot_response,
                title="Mental Health Chatbot",
                examples=["How can I manage stress?", "What are some relaxation techniques?"]
            )

            gr.Markdown("This chatbot provides general support. For urgent issues, seek help from licensed professionals.")

        app.launch(share=True, debug=True)

    except Exception as e:
        logger.error(f"Error during initialization: {str(e)}")
        raise

if __name__ == "__main__":
    main()

# Chatbot Response Function ÏàòÏ†ï
def chatbot_response(user_input, history=[]):
    if not user_input.strip():
        return "Please provide a valid input", history

    try:
        logger.info(f"Processing user input: {user_input}")
        response = qa_chain.run(user_input)

        if not response:
            response = "I apologize, but I couldn't generate a proper response. Could you please rephrase your question?"

        # Î∞òÌôò ÌòïÏãù ÏàòÏ†ï
        history.append({"role": "user", "content": user_input})
        history.append({"role": "assistant", "content": response})
        return response, history
    except Exception as e:
        logger.error(f"Error in chatbot response: {str(e)}")
        error_message = f"I apologize, but an error occurred: {str(e)}"
        history.append({"role": "assistant", "content": error_message})
        return error_message, history

# Gradio Interface ÏàòÏ†ï
def main():
    print("Initializing Chatbot.........")
    try:
        global qa_chain
        llm = initialize_llm()

        db_path = "/content/chroma_db"

        if not os.path.exists(db_path):
            vector_db = create_vector_db()
            if vector_db is None:
                raise Exception("Failed to create vector database")
        else:
            embeddings = HuggingFaceBgeEmbeddings(
                model_name='sentence-transformers/all-MiniLM-L6-v2'
            )
            vector_db = Chroma(
                persist_directory=db_path,
                embedding_function=embeddings
            )

        qa_chain = setup_qa_chain(vector_db, llm)

        # Gradio Interface
        with gr.Blocks(theme='Respair/Shiki@1.2.1') as app:
            gr.Markdown("# üß† Mental Health Chatbot ü§ñ")
            gr.Markdown("A compassionate chatbot designed to assist with mental well-being. Please note: For serious concerns, contact a professional.")

            chatbot = gr.Chatbot()
            msg = gr.Textbox(label="Type your message here...")
            clear = gr.Button("Clear")

            def user(user_message, history):
                return "", history + [[user_message, None]]

            def bot(history):
                user_message = history[-1][0]
                response, _ = chatbot_response(user_message, [])
                history[-1][1] = response
                return history

            msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(
                bot, chatbot, chatbot
            )
            clear.click(lambda: None, None, chatbot, queue=False)

            gr.Markdown("This chatbot provides general support. For urgent issues, seek help from licensed professionals.")

        app.launch(share=True, debug=True)

    except Exception as e:
        logger.error(f"Error during initialization: {str(e)}")
        raise

if __name__ == "__main__":
    main()

!pip install langchain-core

!pip install langchain langchain-groq chromadb sentence-transformers

!pip install -U langchain langchain-groq chromadb sentence-transformers gradio

!pip install langchain langchain-groq chromadb sentence-transformers gradio

GROQ_API_KEY = "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"

!pip install --upgrade gradio

import os
import logging
import gradio as gr
from langchain_groq import ChatGroq
from langchain.memory import ConversationSummaryMemory
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.vectorstores import Chroma

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# Groq API ÌÇ§ ÏÑ§Ï†ï
GROQ_API_KEY = "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"
os.environ["GROQ_API_KEY"] = GROQ_API_KEY

class ChatbotWithMemory:
    def __init__(self):
        self.llm = None
        self.memory = None
        self.qa_chain = None
        self.initialize_components()

    def initialize_components(self):
        try:
            # LLM Ï¥àÍ∏∞Ìôî
            self.llm = ChatGroq(
                temperature=0.7,
                groq_api_key=GROQ_API_KEY,
                model_name="mixtral-8x7b-32768"
            )

            # Î©îÎ™®Î¶¨ Ï¥àÍ∏∞Ìôî
            self.memory = ConversationSummaryMemory(
                llm=self.llm,
                memory_key="chat_history",
                return_messages=True
            )

            # Vector DB Ï¥àÍ∏∞Ìôî
            self.vector_db = self.create_vector_db()

            # QA Chain Ï¥àÍ∏∞Ìôî
            self.qa_chain = self.setup_qa_chain()

        except Exception as e:
            logger.error(f"Initialization error: {str(e)}")
            raise

    def create_vector_db(self):
        try:
            embeddings = HuggingFaceBgeEmbeddings(
                model_name='sentence-transformers/all-MiniLM-L6-v2'
            )

            texts = [
                "Ï†ïÏã† Í±¥Í∞ïÏùÄ Îß§Ïö∞ Ï§ëÏöîÌï©ÎãàÎã§.",
                "ÎèÑÏõÄÏùÑ ÏöîÏ≤≠ÌïòÎäî Í≤ÉÏùÄ Ïö©Í∏∞Ïùò ÌëúÌòÑÏûÖÎãàÎã§.",
                "ÏÉÅÎã¥ÏùÄ Îß§Ïö∞ Ìö®Í≥ºÏ†ÅÏùº Ïàò ÏûàÏäµÎãàÎã§."
            ]

            db_path = "./chroma_db"
            if not os.path.exists(db_path):
                vector_db = Chroma.from_texts(
                    texts=texts,
                    embedding=embeddings,
                    persist_directory=db_path
                )
            else:
                vector_db = Chroma(
                    persist_directory=db_path,
                    embedding_function=embeddings
                )

            return vector_db
        except Exception as e:
            logger.error(f"Vector DB creation error: {str(e)}")
            raise

    def setup_qa_chain(self):
        try:
            retriever = self.vector_db.as_retriever(search_kwargs={"k": 3})

            # ÏàòÏ†ïÎêú ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶ø
            prompt = PromptTemplate(
                template="""Îã§Ïùå Ï†ïÎ≥¥Î•º Î∞îÌÉïÏúºÎ°ú ÏßàÎ¨∏Ïóê ÎãµÎ≥ÄÌï¥Ï£ºÏÑ∏Ïöî.

                Ïª®ÌÖçÏä§Ìä∏: {context}
                Ïù¥Ï†Ñ ÎåÄÌôî: {chat_history}
                ÏßàÎ¨∏: {question}

                ÎãµÎ≥Ä:""",
                input_variables=["context", "chat_history", "question"]
            )

            qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=retriever,
                return_source_documents=True,
                chain_type_kwargs={
                    "prompt": prompt,
                    "verbose": True
                }
            )

            return qa_chain
        except Exception as e:
            logger.error(f"QA Chain setup error: {str(e)}")
            raise

    def get_response(self, user_input, history):
        try:
            if not user_input.strip():
                return history

            # ÎåÄÌôî Í∏∞Î°ù Í∞ÄÏ†∏Ïò§Í∏∞
            chat_history = self.memory.load_memory_variables({}).get("chat_history", "")

            # QA Chain Ïã§Ìñâ
            response = self.qa_chain({
                "question": user_input,
                "chat_history": str(chat_history)
            })

            answer = response.get('result', "Ï£ÑÏÜ°Ìï©ÎãàÎã§. ÏùëÎãµÏùÑ ÏÉùÏÑ±Ìï† Ïàò ÏóÜÏäµÎãàÎã§.")

            # ÎåÄÌôî Ï†ÄÏû•
            self.memory.save_context(
                {"input": user_input},
                {"output": answer}
            )

             # messages ÌòïÏãùÏúºÎ°ú Î≥ÄÌôòÌïòÏó¨ ÌûàÏä§ÌÜ†Î¶¨ ÏóÖÎç∞Ïù¥Ìä∏
            if history is None:
                history = []
            history.append(
                {"role": "user", "content": user_input}
            )
            history.append(
                {"role": "assistant", "content": answer}
            )

            return history

        except Exception as e:
            logger.error(f"Response generation error: {str(e)}")
            if history is None:
                history = []
            history.append(
                {"role": "user", "content": user_input}
            )
            history.append(
                {"role": "assistant", "content": f"Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}"}
            )
            return history

    def clear_history(self):
        self.memory.clear()
        return None

def create_gradio_interface():
    chatbot = ChatbotWithMemory()

    with gr.Blocks(theme=gr.themes.Soft()) as app:
        gr.Markdown("# ü§ñ Mental Health Chatbot")

        chat_interface = gr.Chatbot(
            show_label=True,
            height=600,
            type="messages"  # messages ÌòïÏãù ÏÇ¨Ïö©
        )

        with gr.Row():
            user_input = gr.Textbox(
                show_label=False,
                placeholder="Î©îÏãúÏßÄÎ•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî...",
                container=False
            )
            submit_btn = gr.Button("Ï†ÑÏÜ°", variant="primary")

        clear_btn = gr.Button("ÎåÄÌôî ÎÇ¥Ïö© ÏßÄÏö∞Í∏∞")

        def user(user_message, history):
            if history is None:
                history = []
            history.append({"role": "user", "content": user_message})
            return "", history

        def bot(history):
            if not history:
                return history
            last_user_message = [msg for msg in history if msg["role"] == "user"][-1]["content"]
            history = chatbot.get_response(last_user_message, history)
            return history

        # Ïù¥Î≤§Ìä∏ Ìï∏Îì§Îü¨ Ïó∞Í≤∞
        user_input.submit(
            user,
            [user_input, chat_interface],
            [user_input, chat_interface]
        ).then(
            bot,
            chat_interface,
            chat_interface
        )

        submit_btn.click(
            user,
            [user_input, chat_interface],
            [user_input, chat_interface]
        ).then(
            bot,
            chat_interface,
            chat_interface
        )

        clear_btn.click(
            lambda: None,
            None,
            chat_interface,
            queue=False
        )

    return app
def main():
    print("Starting Chatbot Application...")
    try:
        app = create_gradio_interface()
        app.launch(debug=True, share=True)
    except Exception as e:
        logger.error(f"Application startup error: {str(e)}")
        raise

if __name__ == "__main__":
    main()

